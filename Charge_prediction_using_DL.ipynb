{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Charge_prediction_using_DL.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM7LS37ncOuRJ9NlIE5I+O+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ISHA252001/learningPython/blob/main/Charge_prediction_using_DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install rdkit-pypi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBE99XafHs7p",
        "outputId": "3e766a42-f092-4526-9854-8c73e8a8a13e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdkit-pypi\n",
            "  Downloading rdkit_pypi-2021.9.2.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.6 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.7/dist-packages (from rdkit-pypi) (1.19.5)\n",
            "Installing collected packages: rdkit-pypi\n",
            "Successfully installed rdkit-pypi-2021.9.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExNahTSwHI-U",
        "outputId": "6b5d29c2-8cc5-4e4e-dcd8-3b3490cbf4f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_12 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 34)                2210      \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 16)                560       \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 4)                 68        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 109,846\n",
            "Trainable params: 109,814\n",
            "Non-trainable params: 32\n",
            "_________________________________________________________________\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [1 0 0 ... 1 0 0]\n",
            " [0 1 0 ... 0 1 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 1]\n",
            " [0 0 1 ... 0 0 0]\n",
            " [1 0 0 ... 0 0 0]]\n",
            "['0\\n' '1\\n' '2\\n' '1\\n' '1\\n' '1\\n' '2\\n' '1\\n' '0\\n' '1\\n' '2\\n' '1\\n'\n",
            " '0\\n' '2\\n' '2\\n' '0\\n' '1\\n' '0\\n' '0\\n' '1\\n' '1\\n' '1\\n' '0\\n' '1\\n'\n",
            " '1\\n' '1\\n' '1\\n' '2\\n' '1\\n' '1\\n' '0\\n' '0\\n' '1\\n' '1\\n' '1\\n' '0\\n'\n",
            " '1\\n' '2\\n' '0\\n' '1\\n' '0\\n' '1\\n' '1\\n' '1\\n' '0\\n' '1\\n' '0\\n' '0\\n'\n",
            " '2\\n' '1\\n' '1\\n' '1\\n' '1\\n' '1\\n' '1\\n' '1\\n' '1\\n' '0\\n' '1\\n' '2\\n'\n",
            " '0\\n' '1\\n' '1\\n' '1\\n' '1\\n' '0\\n' '1\\n' '1\\n' '1\\n' '1\\n' '1\\n' '0\\n'\n",
            " '1\\n' '1\\n' '2\\n' '2\\n' '2\\n' '0\\n' '1\\n' '1\\n' '1\\n' '2\\n' '2\\n' '1\\n'\n",
            " '1\\n' '0\\n' '0\\n' '0\\n' '0\\n' '0\\n' '0\\n' '1\\n' '2\\n' '1\\n' '1\\n' '0\\n'\n",
            " '0\\n' '1\\n' '0\\n' '1\\n' '2\\n' '1\\n' '1\\n' '1\\n' '1\\n' '0\\n' '0\\n' '0\\n'\n",
            " '0\\n' '2\\n' '0\\n' '1\\n' '1\\n' '1\\n' '1\\n' '2\\n' '0\\n' '0\\n' '1\\n' '1\\n'\n",
            " '1\\n' '1\\n' '2\\n' '1\\n' '0\\n' '1\\n' '1\\n' '1\\n' '0\\n' '1\\n' '1\\n' '0\\n'\n",
            " '1\\n' '1\\n' '1\\n' '2\\n' '1\\n' '1\\n' '1\\n' '0\\n' '1\\n' '1\\n' '0\\n' '0\\n'\n",
            " '1\\n' '0\\n' '2\\n' '0\\n' '1\\n' '0\\n' '1\\n' '1\\n' '0\\n' '1\\n' '1\\n' '1\\n'\n",
            " '1\\n' '0\\n' '1\\n' '2\\n' '1\\n' '1\\n' '1\\n' '1\\n' '0\\n' '1\\n' '2\\n' '1\\n'\n",
            " '1\\n' '2\\n' '0\\n' '2\\n' '1\\n' '1\\n' '0\\n' '1\\n' '2\\n' '1\\n' '1\\n' '0\\n'\n",
            " '2\\n' '1\\n' '0\\n' '1\\n' '1\\n' '0\\n' '0\\n' '0\\n' '1\\n' '0\\n' '1\\n' '1\\n'\n",
            " '1\\n' '1\\n' '0\\n' '2\\n' '1\\n' '1\\n' '0\\n' '1\\n' '1\\n' '0\\n' '0\\n' '1\\n'\n",
            " '2\\n' '1\\n' '1\\n' '1\\n' '0\\n' '1\\n' '1\\n' '0\\n' '1\\n' '1\\n' '1\\n' '1\\n'\n",
            " '0\\n' '0\\n' '2\\n' '0\\n' '0\\n' '0\\n' '0\\n' '2\\n' '2\\n' '0\\n' '0\\n' '0\\n'\n",
            " '0\\n' '0\\n' '2\\n' '2\\n' '0\\n' '0\\n' '2\\n' '0\\n' '1\\n' '0\\n' '0\\n' '0\\n'\n",
            " '1\\n' '1\\n' '1\\n' '0\\n' '1\\n' '0\\n' '0\\n' '1\\n' '0\\n' '1\\n' '0\\n' '1\\n'\n",
            " '2\\n' '1\\n' '0\\n' '0\\n' '0\\n' '0\\n' '2\\n' '1\\n' '0\\n' '0\\n' '0\\n' '2\\n'\n",
            " '0\\n' '0\\n' '0\\n' '0\\n' '0\\n' '0\\n' '1\\n' '0\\n' '1\\n' '0\\n' '0\\n' '0\\n'\n",
            " '3\\n' '0\\n' '0\\n' '2\\n' '2\\n' '1\\n' '1\\n' '0\\n' '0\\n' '0\\n' '0\\n' '2\\n'\n",
            " '0\\n' '2\\n' '0\\n' '0\\n' '0\\n' '2\\n' '2\\n' '2\\n' '1\\n' '1\\n' '1\\n' '1\\n'\n",
            " '0\\n' '1\\n' '1\\n' '0\\n' '2\\n' '1\\n' '0\\n' '1\\n' '0\\n' '1\\n' '2\\n' '1\\n'\n",
            " '2\\n' '2\\n' '0\\n' '0\\n' '1\\n' '0\\n' '0\\n' '1\\n' '0\\n' '1\\n' '0\\n' '0\\n'\n",
            " '2\\n' '0\\n' '0\\n' '1\\n' '1\\n' '1\\n' '0\\n' '0\\n' '2\\n' '0\\n' '1\\n' '2\\n'\n",
            " '0\\n' '1\\n' '0\\n' '1\\n' '1\\n' '1\\n' '0\\n' '1\\n' '0\\n' '1\\n' '0\\n' '0\\n'\n",
            " '2\\n' '2\\n' '0\\n' '0\\n' '1\\n' '0\\n' '0\\n' '0\\n' '2\\n' '0\\n' '0\\n' '0\\n'\n",
            " '1\\n' '2\\n' '0\\n' '0\\n' '0\\n' '0\\n' '1\\n' '1\\n' '0\\n' '0\\n' '2\\n' '0\\n'\n",
            " '2\\n' '1\\n' '0\\n' '2\\n' '0\\n' '0\\n' '1\\n' '0\\n' '2\\n' '0\\n' '0\\n' '0\\n'\n",
            " '0\\n' '1\\n' '0\\n' '0\\n' '0\\n' '1\\n' '0\\n' '0\\n' '0\\n' '0\\n' '0\\n' '0\\n'\n",
            " '0\\n' '1\\n' '2\\n' '2\\n' '2\\n' '2\\n' '1\\n' '1\\n' '1\\n' '1\\n' '1\\n' '2\\n'\n",
            " '2\\n' '0\\n' '2\\n' '0\\n' '0\\n' '0\\n' '0\\n' '0\\n' '2\\n' '0\\n' '1\\n' '0\\n'\n",
            " '0\\n' '0\\n' '2\\n' '2\\n' '0\\n' '1\\n' '2\\n' '2\\n' '0\\n' '2\\n' '2\\n' '1\\n'\n",
            " '0\\n' '0\\n' '0\\n' '1\\n' '1\\n' '1\\n' '2\\n' '2\\n' '1\\n' '1\\n' '1\\n' '0\\n'\n",
            " '0\\n' '2\\n' '1\\n' '0\\n' '1\\n' '1\\n' '1\\n' '3\\n' '2\\n' '2\\n' '1\\n' '1\\n'\n",
            " '0\\n' '1\\n' '0\\n' '1\\n' '0\\n' '0\\n' '0\\n' '1\\n' '0\\n' '2\\n' '3\\n' '1\\n'\n",
            " '0\\n' '0\\n' '1\\n' '1\\n' '2\\n' '1\\n' '0\\n' '1\\n' '2\\n' '1\\n' '0\\n' '0\\n'\n",
            " '0\\n' '2\\n' '0\\n' '1\\n' '0\\n' '0\\n' '1\\n' '2\\n' '0\\n' '0\\n' '0\\n' '1\\n'\n",
            " '1\\n' '0\\n' '2\\n' '0\\n' '2\\n' '1\\n' '1\\n' '0\\n' '0\\n' '0\\n' '0\\n' '0\\n'\n",
            " '1\\n' '0\\n' '0\\n' '2\\n' '0\\n' '0\\n' '0\\n' '1\\n' '1\\n' '2\\n' '1\\n' '0\\n'\n",
            " '1\\n' '1\\n' '2\\n' '2\\n' '0\\n' '0\\n' '0\\n' '0\\n' '0\\n' '1\\n' '0\\n' '0\\n'\n",
            " '0\\n' '0\\n' '0\\n' '1\\n' '0\\n' '0\\n' '1\\n' '0\\n' '1\\n' '1\\n' '0\\n' '0\\n'\n",
            " '0\\n' '0\\n' '0\\n' '2\\n' '0\\n' '1\\n' '2\\n' '1\\n' '0\\n' '1\\n' '0\\n' '0\\n'\n",
            " '2\\n' '2\\n' '1\\n' '1\\n' '1\\n' '1\\n' '2\\n' '1\\n' '2\\n' '1\\n' '0\\n' '0\\n'\n",
            " '1\\n' '0\\n' '2\\n' '1\\n' '0\\n' '0\\n' '0\\n' '0\\n' '1\\n' '2\\n' '1\\n' '0\\n'\n",
            " '0\\n' '0\\n' '0\\n' '0\\n' '2\\n' '2\\n' '0\\n' '0\\n' '0\\n' '0\\n' '2\\n' '2\\n'\n",
            " '2\\n' '2\\n' '2\\n' '0\\n' '0\\n' '2\\n' '0\\n' '2\\n' '2\\n' '2\\n' '2\\n' '0\\n'\n",
            " '1\\n' '1\\n' '0\\n' '1\\n' '2\\n' '1\\n' '0\\n' '0\\n' '0\\n' '0\\n' '0\\n' '2\\n'\n",
            " '1\\n' '2\\n' '0\\n' '0\\n' '2\\n' '2\\n' '2\\n' '2\\n' '1\\n' '0\\n' '0\\n' '0\\n'\n",
            " '0\\n' '0\\n' '0\\n' '2\\n' '0\\n' '1\\n' '1\\n' '1\\n' '0\\n' '0\\n' '1\\n' '0\\n'\n",
            " '0\\n' '0\\n' '0\\n' '1\\n' '1\\n' '2\\n' '1\\n' '1\\n' '1\\n' '0\\n' '1\\n' '2\\n'\n",
            " '1\\n' '2\\n' '1\\n' '1\\n' '1\\n' '1\\n' '1\\n' '2\\n' '1\\n' '2\\n' '1\\n' '0\\n'\n",
            " '1\\n' '0\\n' '1\\n' '2\\n' '0\\n' '1\\n' '0\\n' '0\\n' '0\\n' '0\\n' '1\\n' '0\\n'\n",
            " '0\\n' '0\\n' '0\\n' '0\\n' '0\\n' '0\\n' '0\\n' '0\\n' '0\\n' '0\\n' '1\\n' '1\\n'\n",
            " '0\\n' '0\\n' '0\\n' '1\\n' '0\\n' '0\\n' '0\\n' '1\\n' '0\\n' '0\\n' '0\\n' '0\\n'\n",
            " '0\\n' '0\\n' '0\\n' '0\\n' '2\\n' '1\\n' '0\\n' '1\\n' '2\\n' '1\\n' '0\\n' '0\\n'\n",
            " '2\\n' '0\\n' '1\\n' '0\\n' '2\\n' '0\\n' '0\\n' '0\\n' '0\\n' '1\\n' '0\\n' '0\\n'\n",
            " '0\\n' '0\\n' '1\\n' '0\\n' '0\\n' '0\\n' '0\\n' '0\\n' '2\\n' '1\\n' '0\\n' '0\\n'\n",
            " '2\\n' '0\\n' '1\\n' '0\\n' '1\\n' '1\\n' '0\\n' '1\\n' '0\\n' '0\\n' '2\\n' '0\\n'\n",
            " '1\\n' '1\\n' '1\\n' '0\\n' '1\\n' '1\\n' '0\\n' '0\\n' '1\\n' '0\\n' '0\\n' '1\\n'\n",
            " '0\\n' '0\\n' '1\\n' '1\\n' '0\\n' '0\\n' '2\\n' '1\\n' '0\\n' '1\\n' '1\\n' '1\\n'\n",
            " '0\\n' '0\\n' '1\\n' '2\\n' '1\\n' '1\\n' '1\\n' '0\\n' '1\\n' '0\\n' '0\\n' '1\\n'\n",
            " '2\\n' '1\\n' '0\\n' '2\\n' '0\\n' '0\\n' '1\\n' '1\\n' '0\\n' '0\\n' '0\\n' '0\\n'\n",
            " '0\\n' '1\\n' '1\\n' '3\\n' '0\\n' '1\\n' '1\\n' '0\\n' '0\\n' '1\\n' '1\\n' '0\\n'\n",
            " '2\\n' '0\\n' '2\\n' '0\\n' '0\\n' '0\\n' '0\\n' '0\\n' '1\\n' '0\\n' '0\\n' '0\\n'\n",
            " '0\\n' '0\\n' '1\\n' '0\\n' '1\\n' '1\\n' '0\\n' '1\\n' '1\\n' '1\\n' '2\\n' '3\\n'\n",
            " '1\\n' '0\\n' '1\\n' '1\\n' '1\\n' '1\\n' '1\\n' '2\\n' '0\\n' '0\\n' '0\\n' '2\\n'\n",
            " '1\\n' '0\\n' '0\\n' '1\\n' '1\\n' '1\\n' '0\\n' '0\\n' '1\\n' '2\\n' '1\\n' '0\\n'\n",
            " '0\\n' '2\\n' '0\\n' '0\\n' '0\\n' '0\\n' '1\\n' '0\\n' '2\\n' '1\\n' '0\\n' '0\\n'\n",
            " '1\\n' '0\\n' '0\\n' '1\\n' '0\\n' '0\\n' '1\\n' '0\\n' '1\\n' '0\\n' '0\\n' '1\\n'\n",
            " '0\\n' '1\\n' '1\\n' '0\\n' '0\\n' '0\\n' '0\\n' '0\\n' '0\\n' '0\\n' '0\\n' '0\\n'\n",
            " '1\\n' '0\\n' '0\\n' '0\\n' '0\\n' '0\\n' '0\\n' '0\\n' '0\\n' '1\\n' '0\\n' '0\\n'\n",
            " '1\\n' '0\\n' '0\\n' '1\\n' '0\\n' '0\\n' '0\\n' '0\\n' '0\\n' '0\\n' '1\\n' '0\\n'\n",
            " '1\\n' '0\\n' '1\\n' '2\\n' '0\\n' '0\\n' '0\\n' '1\\n' '1\\n' '0\\n' '2\\n' '1\\n'\n",
            " '1\\n' '0\\n' '0\\n' '0\\n' '2\\n' '1\\n' '0\\n' '1\\n' '1\\n' '0\\n' '0\\n' '0\\n'\n",
            " '1\\n' '0\\n' '0\\n' '1\\n' '0\\n' '0\\n' '0\\n' '1\\n' '1\\n' '2\\n' '1\\n' '1\\n'\n",
            " '1\\n' '0\\n' '0\\n' '0\\n' '0\\n' '1\\n' '0\\n' '0\\n' '0\\n' '0\\n' '1\\n' '0\\n'\n",
            " '0\\n' '1\\n' '2\\n' '2\\n' '1\\n' '1\\n' '1\\n' '0\\n' '0\\n' '2\\n' '0\\n' '0\\n'\n",
            " '2\\n' '0\\n' '0\\n' '1\\n' '1\\n' '1\\n' '0\\n' '0\\n' '0\\n' '0\\n' '0\\n' '0\\n'\n",
            " '0\\n' '1\\n' '1\\n' '0\\n' '0\\n' '0\\n' '1\\n' '0\\n' '0\\n' '0\\n' '0\\n' '2\\n'\n",
            " '1\\n' '0\\n' '0\\n' '0']\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "81/81 [==============================] - 1s 5ms/step - loss: 1.3848 - accuracy: 0.2593 - val_loss: 1.1979 - val_accuracy: 0.5889\n",
            "Epoch 2/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 1.3801 - accuracy: 0.3247 - val_loss: 1.2255 - val_accuracy: 0.5889\n",
            "Epoch 3/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 1.3760 - accuracy: 0.3494 - val_loss: 1.2601 - val_accuracy: 0.5889\n",
            "Epoch 4/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 1.3715 - accuracy: 0.4407 - val_loss: 1.2912 - val_accuracy: 0.5889\n",
            "Epoch 5/100\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 1.3672 - accuracy: 0.4864 - val_loss: 1.3153 - val_accuracy: 0.5889\n",
            "Epoch 6/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 1.3609 - accuracy: 0.5321 - val_loss: 1.3321 - val_accuracy: 0.5889\n",
            "Epoch 7/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 1.3564 - accuracy: 0.5778 - val_loss: 1.3421 - val_accuracy: 0.6111\n",
            "Epoch 8/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 1.3482 - accuracy: 0.6309 - val_loss: 1.3470 - val_accuracy: 0.7444\n",
            "Epoch 9/100\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 1.3419 - accuracy: 0.6568 - val_loss: 1.3476 - val_accuracy: 0.7222\n",
            "Epoch 10/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 1.3352 - accuracy: 0.6765 - val_loss: 1.3436 - val_accuracy: 0.5222\n",
            "Epoch 11/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 1.3251 - accuracy: 0.7259 - val_loss: 1.3392 - val_accuracy: 0.4778\n",
            "Epoch 12/100\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 1.3158 - accuracy: 0.7346 - val_loss: 1.3306 - val_accuracy: 0.4778\n",
            "Epoch 13/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 1.3058 - accuracy: 0.7235 - val_loss: 1.3225 - val_accuracy: 0.4556\n",
            "Epoch 14/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 1.2919 - accuracy: 0.7457 - val_loss: 1.3123 - val_accuracy: 0.4667\n",
            "Epoch 15/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 1.2764 - accuracy: 0.7506 - val_loss: 1.3003 - val_accuracy: 0.5000\n",
            "Epoch 16/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 1.2617 - accuracy: 0.7519 - val_loss: 1.2878 - val_accuracy: 0.5333\n",
            "Epoch 17/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 1.2425 - accuracy: 0.7593 - val_loss: 1.2726 - val_accuracy: 0.5222\n",
            "Epoch 18/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 1.2213 - accuracy: 0.7716 - val_loss: 1.2562 - val_accuracy: 0.5444\n",
            "Epoch 19/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 1.1985 - accuracy: 0.7642 - val_loss: 1.2384 - val_accuracy: 0.5444\n",
            "Epoch 20/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 1.1732 - accuracy: 0.7753 - val_loss: 1.2190 - val_accuracy: 0.5556\n",
            "Epoch 21/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 1.1506 - accuracy: 0.7728 - val_loss: 1.1985 - val_accuracy: 0.5556\n",
            "Epoch 22/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 1.1238 - accuracy: 0.7691 - val_loss: 1.1764 - val_accuracy: 0.5667\n",
            "Epoch 23/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 1.0972 - accuracy: 0.7617 - val_loss: 1.1534 - val_accuracy: 0.6222\n",
            "Epoch 24/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 1.0679 - accuracy: 0.7815 - val_loss: 1.1302 - val_accuracy: 0.6556\n",
            "Epoch 25/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 1.0575 - accuracy: 0.7728 - val_loss: 1.1071 - val_accuracy: 0.7000\n",
            "Epoch 26/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 1.0154 - accuracy: 0.7951 - val_loss: 1.0851 - val_accuracy: 0.7222\n",
            "Epoch 27/100\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 0.9764 - accuracy: 0.8025 - val_loss: 1.0616 - val_accuracy: 0.7444\n",
            "Epoch 28/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.9727 - accuracy: 0.8025 - val_loss: 1.0398 - val_accuracy: 0.7444\n",
            "Epoch 29/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.9426 - accuracy: 0.7975 - val_loss: 1.0186 - val_accuracy: 0.7667\n",
            "Epoch 30/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.9233 - accuracy: 0.8037 - val_loss: 0.9992 - val_accuracy: 0.7778\n",
            "Epoch 31/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.9036 - accuracy: 0.8198 - val_loss: 0.9797 - val_accuracy: 0.7889\n",
            "Epoch 32/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.8755 - accuracy: 0.8198 - val_loss: 0.9611 - val_accuracy: 0.8111\n",
            "Epoch 33/100\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 0.8803 - accuracy: 0.8247 - val_loss: 0.9423 - val_accuracy: 0.8222\n",
            "Epoch 34/100\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 0.8358 - accuracy: 0.8346 - val_loss: 0.9257 - val_accuracy: 0.8222\n",
            "Epoch 35/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.8230 - accuracy: 0.8333 - val_loss: 0.9105 - val_accuracy: 0.8333\n",
            "Epoch 36/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.8079 - accuracy: 0.8358 - val_loss: 0.8964 - val_accuracy: 0.8333\n",
            "Epoch 37/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.8075 - accuracy: 0.8407 - val_loss: 0.8811 - val_accuracy: 0.8222\n",
            "Epoch 38/100\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 0.8054 - accuracy: 0.8333 - val_loss: 0.8664 - val_accuracy: 0.8222\n",
            "Epoch 39/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.7897 - accuracy: 0.8296 - val_loss: 0.8524 - val_accuracy: 0.8333\n",
            "Epoch 40/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.7480 - accuracy: 0.8432 - val_loss: 0.8390 - val_accuracy: 0.8333\n",
            "Epoch 41/100\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 0.7351 - accuracy: 0.8556 - val_loss: 0.8261 - val_accuracy: 0.8333\n",
            "Epoch 42/100\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 0.7485 - accuracy: 0.8407 - val_loss: 0.8141 - val_accuracy: 0.8333\n",
            "Epoch 43/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.7575 - accuracy: 0.8321 - val_loss: 0.8035 - val_accuracy: 0.8333\n",
            "Epoch 44/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.7145 - accuracy: 0.8420 - val_loss: 0.7912 - val_accuracy: 0.8333\n",
            "Epoch 45/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6891 - accuracy: 0.8679 - val_loss: 0.7797 - val_accuracy: 0.8333\n",
            "Epoch 46/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6798 - accuracy: 0.8481 - val_loss: 0.7697 - val_accuracy: 0.8444\n",
            "Epoch 47/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.8383 - val_loss: 0.7595 - val_accuracy: 0.8444\n",
            "Epoch 48/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6591 - accuracy: 0.8543 - val_loss: 0.7481 - val_accuracy: 0.8444\n",
            "Epoch 49/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6516 - accuracy: 0.8716 - val_loss: 0.7384 - val_accuracy: 0.8444\n",
            "Epoch 50/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6399 - accuracy: 0.8593 - val_loss: 0.7290 - val_accuracy: 0.8444\n",
            "Epoch 51/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6446 - accuracy: 0.8568 - val_loss: 0.7188 - val_accuracy: 0.8444\n",
            "Epoch 52/100\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 0.6212 - accuracy: 0.8519 - val_loss: 0.7091 - val_accuracy: 0.8444\n",
            "Epoch 53/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6336 - accuracy: 0.8630 - val_loss: 0.6983 - val_accuracy: 0.8444\n",
            "Epoch 54/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.5972 - accuracy: 0.8617 - val_loss: 0.6918 - val_accuracy: 0.8444\n",
            "Epoch 55/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.5933 - accuracy: 0.8679 - val_loss: 0.6835 - val_accuracy: 0.8444\n",
            "Epoch 56/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.5769 - accuracy: 0.8864 - val_loss: 0.6735 - val_accuracy: 0.8444\n",
            "Epoch 57/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.5995 - accuracy: 0.8580 - val_loss: 0.6656 - val_accuracy: 0.8444\n",
            "Epoch 58/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.5598 - accuracy: 0.8753 - val_loss: 0.6589 - val_accuracy: 0.8444\n",
            "Epoch 59/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.5641 - accuracy: 0.8877 - val_loss: 0.6518 - val_accuracy: 0.8444\n",
            "Epoch 60/100\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 0.5644 - accuracy: 0.8852 - val_loss: 0.6448 - val_accuracy: 0.8444\n",
            "Epoch 61/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.5583 - accuracy: 0.8815 - val_loss: 0.6381 - val_accuracy: 0.8444\n",
            "Epoch 62/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.5286 - accuracy: 0.8852 - val_loss: 0.6345 - val_accuracy: 0.8444\n",
            "Epoch 63/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.5335 - accuracy: 0.8741 - val_loss: 0.6272 - val_accuracy: 0.8333\n",
            "Epoch 64/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.5293 - accuracy: 0.8765 - val_loss: 0.6223 - val_accuracy: 0.8333\n",
            "Epoch 65/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.5119 - accuracy: 0.8926 - val_loss: 0.6166 - val_accuracy: 0.8333\n",
            "Epoch 66/100\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 0.5108 - accuracy: 0.8778 - val_loss: 0.6120 - val_accuracy: 0.8333\n",
            "Epoch 67/100\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 0.4851 - accuracy: 0.8864 - val_loss: 0.6081 - val_accuracy: 0.8333\n",
            "Epoch 68/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.4953 - accuracy: 0.8938 - val_loss: 0.6019 - val_accuracy: 0.8333\n",
            "Epoch 69/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.4639 - accuracy: 0.8914 - val_loss: 0.5984 - val_accuracy: 0.8333\n",
            "Epoch 70/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.4809 - accuracy: 0.8864 - val_loss: 0.5932 - val_accuracy: 0.8333\n",
            "Epoch 71/100\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 0.4604 - accuracy: 0.8975 - val_loss: 0.5881 - val_accuracy: 0.8333\n",
            "Epoch 72/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.4626 - accuracy: 0.8840 - val_loss: 0.5840 - val_accuracy: 0.8333\n",
            "Epoch 73/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.4514 - accuracy: 0.8975 - val_loss: 0.5814 - val_accuracy: 0.8333\n",
            "Epoch 74/100\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 0.4405 - accuracy: 0.9062 - val_loss: 0.5783 - val_accuracy: 0.8333\n",
            "Epoch 75/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.4445 - accuracy: 0.9062 - val_loss: 0.5745 - val_accuracy: 0.8333\n",
            "Epoch 76/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.4329 - accuracy: 0.9000 - val_loss: 0.5704 - val_accuracy: 0.8333\n",
            "Epoch 77/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.4421 - accuracy: 0.8975 - val_loss: 0.5653 - val_accuracy: 0.8333\n",
            "Epoch 78/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.4359 - accuracy: 0.9037 - val_loss: 0.5639 - val_accuracy: 0.8333\n",
            "Epoch 79/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.4329 - accuracy: 0.9037 - val_loss: 0.5604 - val_accuracy: 0.8333\n",
            "Epoch 80/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.4066 - accuracy: 0.9074 - val_loss: 0.5570 - val_accuracy: 0.8222\n",
            "Epoch 81/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.4130 - accuracy: 0.9012 - val_loss: 0.5534 - val_accuracy: 0.8333\n",
            "Epoch 82/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.4232 - accuracy: 0.9062 - val_loss: 0.5514 - val_accuracy: 0.8222\n",
            "Epoch 83/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.3996 - accuracy: 0.9074 - val_loss: 0.5478 - val_accuracy: 0.8333\n",
            "Epoch 84/100\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 0.3865 - accuracy: 0.9235 - val_loss: 0.5482 - val_accuracy: 0.8333\n",
            "Epoch 85/100\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 0.4003 - accuracy: 0.9111 - val_loss: 0.5447 - val_accuracy: 0.8333\n",
            "Epoch 86/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.3711 - accuracy: 0.9160 - val_loss: 0.5421 - val_accuracy: 0.8333\n",
            "Epoch 87/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.3913 - accuracy: 0.9123 - val_loss: 0.5413 - val_accuracy: 0.8333\n",
            "Epoch 88/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.3942 - accuracy: 0.9136 - val_loss: 0.5385 - val_accuracy: 0.8333\n",
            "Epoch 89/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.3609 - accuracy: 0.9185 - val_loss: 0.5352 - val_accuracy: 0.8333\n",
            "Epoch 90/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.3898 - accuracy: 0.9062 - val_loss: 0.5330 - val_accuracy: 0.8222\n",
            "Epoch 91/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.3902 - accuracy: 0.9062 - val_loss: 0.5293 - val_accuracy: 0.8333\n",
            "Epoch 92/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.3535 - accuracy: 0.9136 - val_loss: 0.5288 - val_accuracy: 0.8222\n",
            "Epoch 93/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.3638 - accuracy: 0.9173 - val_loss: 0.5275 - val_accuracy: 0.8222\n",
            "Epoch 94/100\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 0.3338 - accuracy: 0.9333 - val_loss: 0.5244 - val_accuracy: 0.8222\n",
            "Epoch 95/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.3518 - accuracy: 0.9247 - val_loss: 0.5225 - val_accuracy: 0.8222\n",
            "Epoch 96/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.3435 - accuracy: 0.9272 - val_loss: 0.5234 - val_accuracy: 0.8222\n",
            "Epoch 97/100\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 0.3602 - accuracy: 0.9123 - val_loss: 0.5171 - val_accuracy: 0.8222\n",
            "Epoch 98/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.3664 - accuracy: 0.9025 - val_loss: 0.5160 - val_accuracy: 0.8222\n",
            "Epoch 99/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.3385 - accuracy: 0.9198 - val_loss: 0.5144 - val_accuracy: 0.8222\n",
            "Epoch 100/100\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 0.3088 - accuracy: 0.9395 - val_loss: 0.5124 - val_accuracy: 0.8222\n",
            "10/10 [==============================] - 0s 2ms/step\n",
            "class\t0\t1\t\t2\t\t3\n",
            "charge\t0\t+1\t\t-1\t\t-2\n",
            "[0.4692823  0.09128825 0.27548182 0.1639476 ]       Charge:  0\n",
            "[0.93132496 0.0034549  0.03788591 0.02733416]       Charge:  0\n",
            "[0.37939924 0.10366179 0.34526762 0.17167144]       Charge:  0\n",
            "[0.41779083 0.13740073 0.26644024 0.17836824]       Charge:  0\n",
            "[0.872322   0.02848369 0.03292432 0.06626996]       Charge:  0\n",
            "[0.00662576 0.01335392 0.9572601  0.02276022]       Charge:  -1\n",
            "[0.16473432 0.6307462  0.03410559 0.17041384]       Charge:  1\n",
            "[0.60980463 0.0762238  0.17343557 0.14053597]       Charge:  0\n",
            "[0.9351933  0.00393312 0.03295539 0.02791822]       Charge:  0\n",
            "[9.8106796e-01 2.2438735e-04 1.2610516e-02 6.0971105e-03]       Charge:  0\n"
          ]
        }
      ],
      "source": [
        "import keras as k\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, BatchNormalization\n",
        "from keras.layers.core import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.metrics import categorical_crossentropy\n",
        "from rdkit import Chem, DataStructs\n",
        "from rdkit.Chem import rdMolDescriptors, Draw\n",
        "from rdkit.Chem.Draw import IPythonConsole\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import math\n",
        "\n",
        "\"\"\" TRAINING DATA PREPROCESSING \"\"\"\n",
        "#Convert training data, in the form of txt files of line-by-line SMILES strings and charges into arrays\n",
        "with open('1K_SMILES_strings.txt') as my_file:\n",
        "    SMILES_array = my_file.readlines()\n",
        "\n",
        "with open('1K_SMILES_strings_charges.txt') as my_file:\n",
        "    charges_array = my_file.readlines()\n",
        "\n",
        "#Convert testing data, in the form of txt file of line-by-line SMILES strings into arrays\n",
        "with open('10_SMILES_strings_test.txt') as my_file:\n",
        "    test_SMILES_array = my_file.readlines()\n",
        "\n",
        "#Convert each item of the training array of SMILES strings into molecules\n",
        "mols = [Chem.rdmolfiles.MolFromSmiles(SMILES_string) for SMILES_string in SMILES_array]\n",
        "\n",
        "#Convert training molecules into training fingerprints\n",
        "bi = {}\n",
        "fps = [rdMolDescriptors.GetMorganFingerprintAsBitVect(m, radius=2, bitInfo= bi, nBits=256) for m in mols]\n",
        "\n",
        "#Convert training fingerprints into binary, and put all training binaries into arrays\n",
        "np_fps_array = []\n",
        "for fp in fps:\n",
        "  arr = np.zeros((1,), dtype= int)\n",
        "  DataStructs.ConvertToNumpyArray(fp, arr)\n",
        "  np_fps_array.append(arr)\n",
        "\n",
        "\"\"\" TESTING DATA PREPROCESSING \"\"\"\n",
        "#Convert each item of the testing array of SMILES strings into molecules\n",
        "test_mols = [Chem.rdmolfiles.MolFromSmiles(test_SMILES_string) for test_SMILES_string in test_SMILES_array]\n",
        "\n",
        "#Convert testing molecules into testing fingerprints\n",
        "test_fps = [rdMolDescriptors.GetMorganFingerprintAsBitVect(test_m, radius=2, nBits=256) for test_m in test_mols]\n",
        "\n",
        "#Convert testing fingerprints into binary, and put all testing binaries into arrays\n",
        "test_np_fps_array = []\n",
        "for test_fp in test_fps:\n",
        "  test_arr = np.zeros((1,), dtype= int)\n",
        "  DataStructs.ConvertToNumpyArray(test_fp, test_arr)\n",
        "  test_np_fps_array.append(test_arr)\n",
        "  \n",
        "\n",
        "\"\"\"NEURAL NETWORK\"\"\"\n",
        "#The neural network model\n",
        "model = Sequential([\n",
        "    Dense(256, input_shape=(256,), activation= \"relu\"),\n",
        "    Dense(128, activation= \"sigmoid\"),\n",
        "    Dense(64, activation= \"sigmoid\"),\n",
        "    Dense(34, activation= \"sigmoid\"),\n",
        "    Dense(16, activation= \"sigmoid\"),\n",
        "    BatchNormalization(axis=1),\n",
        "    Dense(4, activation= \"softmax\")\n",
        "])\n",
        "model.summary()\n",
        "\n",
        "#Compiling the model\n",
        "model.compile(optimizer=Adam(lr=0.00001), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "print(np.array(np_fps_array))\n",
        "print(np.array(charges_array))\n",
        "\n",
        "charges_array_int=[]\n",
        "for i in range(len(charges_array)):\n",
        "  charges_array_int.append(int(charges_array[i][0]))\n",
        "\n",
        "#Training the model\n",
        "model.fit(np.array(np_fps_array), np.array(charges_array_int), validation_split=0.1, batch_size=10, epochs= 100, shuffle=True, verbose=1)\n",
        "\n",
        "#Predictions with test dataset\n",
        "predictions = model.predict(np.array(test_np_fps_array), batch_size=1, verbose=1)\n",
        "\n",
        "print('class\\t0\\t1\\t\\t2\\t\\t3')\n",
        "print('charge\\t0\\t+1\\t\\t-1\\t\\t-2')\n",
        "charges = [0,1,-1,-2]\n",
        "for prediction in predictions:\n",
        "    print (prediction, \"      Charge: \", charges[np.argmax(prediction)])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sQW85x72HqVw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}